# -*- coding: utf-8 -*-
"""AS1_1771078_정드림.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MRhTqB3-eB_h-tfZGYaaLfBZbASWOzeL

#Assignment 1: Logistic Regression

## Dataset load & Plot
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.linear_model import LogisticRegression
from warnings import filterwarnings
filterwarnings('ignore')

data = np.loadtxt('data.csv', delimiter=',')
X = data[:, :2]
y = data[:, 2]
label_mask = np.equal(y, 1)

plt.scatter(X[:, 0][label_mask], X[:, 1][label_mask], color='red')
plt.scatter(X[:, 0][~label_mask], X[:, 1][~label_mask], color='blue')
plt.show()

"""# Problem 1-1. Logistic regression model using sklearn packages.

Train your data using LogisticRegression classes from skikit-learn library.

"""

def learn_and_return_weights(X, y):
    from sklearn.linear_model import LogisticRegression
    model=LogisticRegression(solver='liblinear',random_state=20)
    model.fit(X,y)
    w=model.coef_[0]
    b=model.intercept_[0]

    return w, b

def plot_data_and_weights(X, y, w, b):
    plt.scatter(X[:, 0][label_mask], X[:, 1][label_mask], color='red')
    plt.scatter(X[:, 0][~label_mask], X[:, 1][~label_mask], color='blue')

    x_lin = np.arange(20, 70)
    y_lin = -(0.5 + b + w[0] * x_lin) / w[1]

    plt.plot(x_lin, y_lin, color='black') #decision boundary
    plt.show()

w, b = learn_and_return_weights(X, y)
plot_data_and_weights(X, y, w, b)

"""## Problem 1-2. Implement Logistic Regression Model

Implement Logistic Regression without using scikit-learn libraries.

"""

y=y.reshape(y.size,1)
X_data=X.T
X_data=np.vstack((np.ones(11),X_data)) #항상 1의 값을 갖는 bias열을 더해준다.
X_data=X_data.T

print(X_data.shape)
print(y.shape)

def sigmoid(z): #output이 0~1 사이의 값을 가지며 soft한 함수
    return 1.0/(1+np.exp(-z))
     
def predict(X,W): #h(x) : θ(S)
  z=np.dot(X,W)
  return sigmoid(z)

def binary_cross_entropy_loss(y_pred, target):
    delta=1e-7
    m=11
    loss=-np.sum(target*np.log(y_pred+delta)+(1-target)*np.log(1-y_pred+delta))/m
    return loss

def learn_and_return_weights_numpy(X, Y, lr=.001, iter=500000):
    W = np.random.rand(3,1)
    
    for i in range(iter):
      pred=predict(X,W)
      gra=np.dot(X.T,pred-Y)/len(X) #gradient : direction
      W-=lr*gra
      loss=binary_cross_entropy_loss(pred,Y)

      if i%50000 ==0:
        print('iter:'+str(i)+', loss:'+str(loss))

    w=W[1:3]
    b=W[0]
    return w,b

w, b = learn_and_return_weights_numpy(X_data, y)
plot_data_and_weights(X, y, w, b)